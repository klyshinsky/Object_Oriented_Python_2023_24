{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Знакомство</h2>\n",
    "\n",
    "Меня зовут Клышинский Эдуард Станиславович. <br>\n",
    "В течение этого года мы будем заниматься изучением возможностей языка Питон, а также библиотек для него.<br>\n",
    "Со мной можно связаться по почте eklyshinsky@hse.ru<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Морфологический анализ</h2>\n",
    "\n",
    "На начальных этапах обработки текста проводится два этапа анализа: <b>графематический</b> (выделение предложений и слов) и <b>морфологический</b> (определение начальной формы слова, его части речи и грамматических параметров). Этап синтаксического анализа мы сейчас разбирать не будем, так как его информация требуется не всегда.<br>\n",
    "Задачей графематического анализа является разделение текста на составные части - врезки, абзацы, предложения, слова. В таких задачах как машинный перевод, точность данного этапа может существенно влиять на точность получаемых результатов. Например, точка, используемая для сокращений, может быть воспринята как конец предложения, что полность разорвет его семантику.<br>\n",
    "Задачей морфологического анализа является определение начальной формы слова, его части речи и грамматических параметров. В некоторых случаях от слова требуется только начальная форма, в других - только начальная форма и часть речи.<br>\n",
    "Существует два больших подхода к морфологическому анализу: <b>стемминг</b> и <b>поиск по словарю</b>. Для проведения стемминга оставляется справочник всех окончаний для данного языка. Для пришедшего слова проверяется его окончание и по нему делается прогноз начальной формы и части речи.<br>\n",
    "Например, мы создаем справочник, в котором записываем все окончания прилагательных: <i>-ому, -ему, -ой, -ая, -ий, -ый, ...</i> Теперь все слова, которые имеют такое окончание будут считаться прилагаельными: <i>синий, циклический, красного, больному</i>. Заодно прилагательными будут считаться причастия (<i>делающий, строившему</i>) и местоимения (<i>мой, твой, твоему</i>). Также не понятно что делать со словами, имеющими пустое окончание. Отдельную проблему составляют такие слова, как <i>стекло, больной, вина</i>, которые могут разбираться несколькими вариантами (это явление называется <b>омонимией</b>). Помимо этого, стеммер может просто откусывать окончания, оставляя лишь псевдооснову.<br>\n",
    "Большинство проблем здесь решается, но точность работы бессловарных стеммеров находится на уровне 80%. Чтобы повысить точность испольуют морфологический анализ со словарем. Разработчики составляют словарь слов, встретившихся в текстах (<a href=\"http://opencorpora.org/dict.php\">здесь</a> можно найти пример такого словаря). Теперь каждое слово будет искаться в словаре и не предсказываться, а выдаваться точно. Для слов, отсутствующих в словаре, может применяться предсказание, пообное работе стеммера.<br>\n",
    "Посмотрим как работает словарная морфология на примере системы <a href=\"https://pymorphy2.readthedocs.io/en/latest/\">pymorphy2</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2 # Импортируем морфологический анализатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='стекло', score=0.690476, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 0),))\n",
      "Parse(word='стекло', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='стекло', score=0.285714, methods_stack=((DictionaryAnalyzer(), 'стекло', 157, 3),))\n",
      "Parse(word='стекло', tag=OpencorporaTag('VERB,perf,intr neut,sing,past,indc'), normal_form='стечь', score=0.023809, methods_stack=((DictionaryAnalyzer(), 'стекло', 1015, 3),))\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь.\n",
    "wordform = morph.parse('стекло')  # Проведем анализ слова \"стекло\".\n",
    "for w in wordform: # Возвращается список вариантов разбора.\n",
    "    print(w) # Посмотрим на полученный результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "стекло 0.285714\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer() # Создает объект морфоанализатора и загружет словарь.\n",
    "wordform = morph.parse('стекло')  # Проведем анализ слова \"стекло\".\n",
    "for w in wordform: # Возвращается список вариантов разбора.\n",
    "    if {'NOUN', 'accs'} in w.tag: # Возьмем только варианты разбора как существительного.\n",
    "        print(w.normal_form, w.score)# Посмотрим на начальные формы в анализе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='делавшихся', tag=OpencorporaTag('PRTF,impf,intr,past,actv plur,gent'), normal_form='делаться', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'делавшихся', 234, 61),)),\n",
       " Parse(word='делавшихся', tag=OpencorporaTag('PRTF,impf,intr,past,actv anim,plur,accs'), normal_form='делаться', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'делавшихся', 234, 63),)),\n",
       " Parse(word='делавшихся', tag=OpencorporaTag('PRTF,impf,intr,past,actv plur,loct'), normal_form='делаться', score=0.3333333333333333, methods_stack=((DictionaryAnalyzer(), 'делавшихся', 234, 66),))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('делавшихся')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='варкалось', tag=OpencorporaTag('NOUN,anim,masc sing,nomn'), normal_form='варкалось', score=0.5000531180282588, methods_stack=((DictionaryAnalyzer(), 'лось', 123, 0), (UnknownPrefixAnalyzer(score_multiplier=0.5), 'варка'))),\n",
       " Parse(word='варкалось', tag=OpencorporaTag('VERB,impf,intr neut,sing,past,indc'), normal_form='варкаться', score=0.4999468819717412, methods_stack=((FakeDictionary(), 'варкалось', 234, 9), (KnownSuffixAnalyzer(min_word_length=4, score_multiplier=0.5), 'алось')))]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('варкалось')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из вывода, слово \"стекло\" может быть неодушевленным существительным среднего рода, единственного числа, именительного падежа <i>tag=OpencorporaTag('NOUN,inan,neut sing,nomn')</i>, аналогично, но в винительном падеже (<i>'NOUN,inan,neut sing,accs'</i>), и глаголом <i>'VERB,perf,intr neut,sing,past,indc'</i>. При этом в первой форме оно встречается в 75% случаев (<i>score=0.75</i>), во второй в 18,75% случаев (<i>score=0.1875</i>), а как глагол - лишь в 6,25% (<i>score=0.0625</i>). Самым простым видом борьбы с омонимией является выбор нулевого элемента из списка, возвращенного морфологическим анализом. Такой подход дает около 90% точности при выборе начальной формы и до 80% если мы обращаем внимание на грамматические параметры.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вместо Pymorphy можно использовать PyMystem. Его плюсом является тот факт, что он сам проводит графематический анализ и снимает омонимию. Используя функцию lemmatize можно получить набор начальных форм слов. Используя функцию analyze можно получить полную информацию о словах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `lemmatize` делит текст на слова и знаки препинания, а затем возвращает для них только начальную форму.\n",
    "\n",
    "Функция `analyze` возвращает не только начальную форму, но и всю информацию о слове, как это делал перед этим Pymorphy. \n",
    "\n",
    "Основным отличием является то, что Mystem снимает омонимию. Как видно из примера, делает он это не всегда корректно, но нам не придется думать о том, какое вариант разбора следует взять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['этот', ' ', 'тип', ' ', 'становиться', ' ', 'есть', ' ', 'в', ' ', 'цех', '\\n']\n",
      "-----\n",
      "{'analysis': [{'lex': 'этот', 'wt': 1, 'gr': 'APRO=(им,мн|вин,мн,неод)'}], 'text': 'эти'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'тип', 'wt': 0.8700298642, 'gr': 'S,муж,неод=(вин,мн|им,мн)'}], 'text': 'типы'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'становиться', 'wt': 0.9821285244, 'gr': 'V,нп=прош,мн,изъяв,сов'}], 'text': 'стали'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'есть', 'wt': 0.0492236161, 'gr': 'V,несов,пе=инф'}], 'text': 'есть'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'в', 'wt': 0.9999917878, 'gr': 'PR='}], 'text': 'в'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'цех', 'wt': 1, 'gr': 'S,муж,неод=(дат,ед|местн,ед)'}], 'text': 'цеху'}\n",
      "{'text': '\\n'}\n"
     ]
    }
   ],
   "source": [
    "phrase = 'эти типы стали есть в цеху'\n",
    "mystem = pymystem3.Mystem()\n",
    "print(mystem.lemmatize(phrase)) # lemmatize возвращает только начальные формы.\n",
    "print('-----')\n",
    "for word in mystem.analyze(phrase): # analyze возвращает полный разбор.\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysis': [{'lex': 'система', 'wt': 1, 'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'Системы'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'автоматизированный', 'wt': 0.6585352169, 'gr': 'A,полн=(вин,ед,муж,од|род,ед,муж|род,ед,сред)'}], 'text': 'автоматизированного'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'проектирование', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'проектирования'}\n",
      "{'text': '\\n'}\n",
      "-----\n",
      "{'analysis': [{'lex': 'система', 'wt': 1, 'gr': 'S,жен,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'Системы'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'сделать', 'wt': 1, 'gr': 'V,сов,пе=(прош,вин,ед,прич,полн,муж,страд,од|прош,род,ед,прич,полн,муж,страд|прош,род,ед,прич,полн,сред,страд)'}], 'text': 'сделанного'}\n",
      "{'text': ' '}\n",
      "{'analysis': [{'lex': 'проектирование', 'wt': 1, 'gr': 'S,сред,неод=(вин,мн|род,ед|им,мн)'}], 'text': 'проектирования'}\n",
      "{'text': '\\n'}\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на варианты разбора фраз\n",
    "phrase = 'Системы автоматизированного проектирования' # ... и обнаружим ошибку.\n",
    "for word in mystem.analyze(phrase): \n",
    "    print(word)\n",
    "print('-----')\n",
    "phrase = 'Системы сделанного проектирования' \n",
    "for word in mystem.analyze(phrase): \n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще одна библиотека - NLTK. По сравнению с двумя предыдущими библиотеками она обладает более широкой функциональностью и изначально писалась для работы с разными языками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # Иностранный морфологический анализатор."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед началом использования необходимо загрузить необходимые библиотеки или корпуса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download() # По дороге будут появляться поле ввода. Грузит всё из Сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/edward/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/edward/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]     /home/edward/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_ru is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to /home/edward/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') # Сразу грузит что попросили.\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_ru')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `word_tokenize` возвращает начальные формы слов. \n",
    "\n",
    "Функция `pos_tag` возвращает список начальных форм и их частей речи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Эти', 'типы', 'стали', 'есть', 'в', 'цеху'],\n",
       " [('Эти', 'типы'),\n",
       "  ('типы', 'стали'),\n",
       "  ('стали', 'есть'),\n",
       "  ('есть', 'в'),\n",
       "  ('в', 'цеху')])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Токенизация. Надо не забыть сказать, что анализируем русский язык.\n",
    "tokens = nltk.word_tokenize('Эти типы стали есть в цеху', language='russian') \n",
    "bi_tokens = list(nltk.bigrams(tokens))\n",
    "tokens, bi_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('Эти', 'A-PRO=pl'),\n",
       "  ('типы', 'S'),\n",
       "  ('стали', 'V'),\n",
       "  ('есть', 'V'),\n",
       "  ('в', 'PR'),\n",
       "  ('цеху', 'S')],\n",
       " [(('Эти', 'A-PRO=pl'), ('типы', 'S')),\n",
       "  (('типы', 'S'), ('стали', 'V')),\n",
       "  (('стали', 'V'), ('есть', 'V')),\n",
       "  (('есть', 'V'), ('в', 'PR')),\n",
       "  (('в', 'PR'), ('цеху', 'S'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = nltk.pos_tag(tokens, lang='rus') # Частеречная разметка.\n",
    "bi_pos = list(nltk.bigrams(pos))\n",
    "pos, bi_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У NLTK заведен список стоп-слов, которые лучше фильтровать при анализе текстов. Но их не очень много. Зато самые мешающиеся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "всего русских стоп-слов 151\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Эти', 'типы', 'стали', 'цеху']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Оставим только те слова, которых нет в списке стоп-слов.\n",
    "filtered_words = [token for token in tokens \n",
    "                  if token not in nltk.corpus.stopwords.words('russian')]\n",
    "print('всего русских стоп-слов', len(nltk.corpus.stopwords.words('russian')))\n",
    "filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже приведены примеры функций морфологического анализа текста для разных библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Подгружаем библиотеку с регулярными выражениями.\n",
    "\n",
    "# Для определения типов параметров функций нам потребуется простой питоновский ...\n",
    "from typing import Optional\n",
    "from pymorphy2.analyzer import MorphAnalyzer\n",
    "from pymystem3.mystem import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('123z123', '123')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'(([0-9]+)z\\2)','asdf123 asd3213 sdfd 4324 fsdfsd 543 34 123z123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['милый_ADJF', 'мама_NOUN', 'мыло_NOUN', 'белый_ADJF', 'рама_NOUN']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pymorphy\n",
    "def normalizePymorphy(morph: MorphAnalyzer, text: str) -> list:\n",
    "    tokens = re.findall('[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+', text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        pv = morph.parse(t)\n",
    "        words.append(f'{pv[0].normal_form}_{str(pv[0].tag.POS)}') # Берем наиболее вероятную форму.\n",
    "    return words    \n",
    "        \n",
    "# Обратите внимание, что про иностранные слова словарь ничего не знает.\n",
    "normalizePymorphy(morph, 'Милая мама мыла белую раму.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['милый_A', 'мама_S', 'мыло_S', 'белый_A', 'рама_S']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyMystem\n",
    "def normalizePymystem(mystem: Mystem, text: str) -> list:\n",
    "    tokens = mystem.analyze(text)\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        if 'analysis' in t.keys():\n",
    "            if t['analysis'] != []:\n",
    "                words.append(f\"{t['analysis'][0]['lex']}_{t['analysis'][0]['gr'][0]}\")\n",
    "            else:\n",
    "                words.append(f\"{t['text']}_U\")\n",
    "    return words    \n",
    "        \n",
    "# Не все считают, что причастие всегда выступает в роли глагола, но иногда так значительно проще.\n",
    "normalizePymystem(mystem, \"Милая мама мыла белую раму.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Милая_A=f', 'мама_S', 'мыла_V', 'белую_A=f', 'раму_S', '._NONLEX']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK\n",
    "def normalizeNLTK(text):\n",
    "    tokens = nltk.pos_tag(nltk.word_tokenize(text), lang='rus')\n",
    "    words = []\n",
    "    for t in tokens:\n",
    "        if t[0] != t[1]:\n",
    "            words.append(f'{t[0]}_{t[1]}')\n",
    "    return words    \n",
    "        \n",
    "# А вот здесь с частеречной разметкой всё плохо, а параметров нет вовсе.\n",
    "normalizeNLTK(\"Милая мама мыла белую раму.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Но мы будем использовать pymorphy, так как он немного пошустрее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим с какой скоростью работают эти анализаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/war_and_peace.txt') as fil:\n",
    "    textWP = fil.read()\n",
    "# Выделяем все слова написанные русской кириллицей.\n",
    "words = [w[0] for w in re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textWP)]\n",
    "newtext = ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"\\n            Лев Николаевич Толстой. Война и мир. Том 1\\n\\n             * ЧАСТЬ ПЕРВАЯ. * \\n\\n\\n          \\n          \\n\\n            I.\\n\\n\\n          \\n               -- Еh bien, mon prince. Gênes et Lucques ne sont plus que des apanages,\\n          des поместья, de la famille Buonaparte.  Non, je  vous préviens, que si vous\\n          ne  me dites pas, que nous avons la guerre, si vous vous permettez encore de\\n          pallier  toutes les infamies, toutes les  atrocités  de cet  Antichrist  (ma\\n          parole, j'y  crois) -- je  ne  vous  connais plus, vous n'êtes plus mon ami,\\n          vous n'êtes  plus  мой  верный  раб,  comme  vous  dites.  [1]  Ну,\\n          здравствуйте, здравствуйте.  Je vois  que  je  vous fais  peur, [2]\\n          садитесь и рассказывайте.\\n               Так говорила в и\",\n",
       " 'Лев Николаевич Толстой Война и мир Том ЧАСТЬ ПЕРВАЯ Е поместья мой верный раб Ну здравствуйте здравствуйте садитесь и рассказывайте Так говорила в июле года известная Анна Павловна Шерер фрейлина и приближенная императрицы Марии Феодоровны встречая важного и чиновного князя Василия первого приехавшего на ее вечер Анна Павловна кашляла несколько дней у нее был грипп как она говорила грипп был тогда новое слово употреблявшееся только редкими В записочках разосланных утром с красным лакеем было написано без различия во всех или отвечал нисколько не смутясь такою встречей вошедший князь в придворном шитом мундире в чулках башмаках при звездах с светлым выражением плоского лица Он говорил на том изысканном французском языке на котором не только говорили но и думали наши деды и с теми тихими пок')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textWP[:800], newtext[:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Лев', ''),\n",
       " ('Николаевич', ''),\n",
       " ('Толстой', ''),\n",
       " ('Война', ''),\n",
       " ('и', ''),\n",
       " ('мир', ''),\n",
       " ('Том', ''),\n",
       " ('ЧАСТЬ', ''),\n",
       " ('ПЕРВАЯ', ''),\n",
       " ('Е', '')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)', textWP)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445508\n"
     ]
    }
   ],
   "source": [
    "print(len(words)) # Вся \"Война и мир\" занимает примерно 450 000 слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.32 s, sys: 6.73 ms, total: 7.33 s\n",
      "Wall time: 7.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iniMorphy = []\n",
    "for w in words:\n",
    "    r = morph.parse(w)\n",
    "    iniMorphy.append(r[0].normal_form) # Берем только начальные формы слов, остальное нам не надо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Может быть это всё долгая работа списка?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.29 s, sys: 0 ns, total: 7.29 s\n",
      "Wall time: 7.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iniMorphy = []\n",
    "for w in words:\n",
    "    r = morph.parse(w)\n",
    "#     iniMorphy.append(r[0].normal_form) # Берем только начальные формы слов, остальное нам не надо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим на скорость работы PyMyStem.\n",
    "\n",
    "!!! Внимание, высокое потребление памяти!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 27s, sys: 17.2 s, total: 2min 45s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iniStem = mystem.lemmatize(newtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "iniStem = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 11.6 s, total: 1min 59s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iniStem = mystem.analyze(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Борьба за производительность</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Примерно 50 000 слов в секунду для PyMorphy и 25 000 (со снятием омонимии) для MyStem.<br>\n",
    "Хорошо, но хочется быстрее.<br>\n",
    "Давайте посмотрим на статистику встречаемости слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter # Импортируем счетчик из стандартных библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "и : 20324\n",
      "в : 10202\n",
      "не : 8434\n",
      "что : 7298\n",
      "на : 6441\n",
      "он : 5986\n",
      "с : 5748\n",
      "его : 3864\n",
      "как : 3681\n",
      "к : 3408\n"
     ]
    }
   ],
   "source": [
    "frqs = Counter(words) # Считаем сколько раз встречается каждое слово, ...\n",
    "frqsSorted = sorted(frqs.items(), key=lambda x: x[1], reverse=True) # ... сортируем, ...\n",
    "for x in frqsSorted[:10]: # ... и выводим 10 самых частоных слов.\n",
    "    print(f'{x[0]} : {frqs[x[0]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16921357192238973\n",
      "0.3676791438088654\n"
     ]
    }
   ],
   "source": [
    "print(sum([x[1] for x in frqsSorted[:10]]) / len(words))\n",
    "print(sum([x[1] for x in frqsSorted[:100]]) / len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сто самых частотных словоформ занимает примерно треть текста!<br>\n",
    "Возможно, если мы сумеем кешировать результаты работы морфологического анализатора, он начнет работать быстрее. Используем для этого обычный питоновский словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecaf1ac430fb4596befc62369ef2c880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(10000)):\n",
    "    sleep(0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 857 ms, sys: 3.89 ms, total: 861 ms\n",
      "Wall time: 862 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "iniMorphy2 = []\n",
    "cash = {} # Словарь для кеширования.\n",
    "for w in words:\n",
    "    if w in cash.keys(): # Если слово было закешировано, возьмем его из словаря.\n",
    "        iniMorphy2.append(cash[w])\n",
    "    else: # В противном случае проведем морфологический анализ.\n",
    "        r = morph.parse(w)\n",
    "        iniMorphy2.append(r[0].normal_form)\n",
    "        cash[w] = r[0].normal_form \n",
    "    # Вообще-то, можно было кешировать все результаты. Но нам же нужна только наиболее вероятная начальная форма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ускорение почти в 8 раз!<br>\n",
    "Имеет смысл сделать из этого какую-то удобную обертку, чтобы повторно использовать в своих дальнейших разработках."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание и использование классов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим свой собственный класс.<br>\n",
    "Класс - это тип, определенный пользователем (программистом). Класс содержит в себе как данные, так и функции, которые работают с этими данными.<br>\n",
    "Так как это тип, то можно создавать переменные этого типа. Каждая переменная будет хранить и обрабатывать свой набор данных.<br>\n",
    "В каждую функцию класса обязательно передается переменная, которая обычно называется self. Эта переменная содержит в себе объект, для которого производится вызов функции. Мы можем обращаться к свойствам данного объекта, используя или модифицируя тем самым этот объект."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ключевое слово class после которого идет название нашего класса.\n",
    "class FasterMorphology:\n",
    "    \"\"\"Class description.\"\"\"\n",
    "    \n",
    "    __morpho: MorphAnalyzer\n",
    "    __cash: dict\n",
    "        \n",
    "    def __init__(self) -> None: # Функция инициализации объекта после его создания.\n",
    "        \"\"\"Initializes an object\"\"\"\n",
    "        # Создаем новую морфологию в каждом объекте. \n",
    "        # А вдруг мы будем потом работать с разными языками? У каждого объекта должна быть своя.\n",
    "        self.__morpho = MorphAnalyzer() \n",
    "        self.__cash = {} # Создаем словарь для кеширования.\n",
    "        \n",
    "    def analyzeWords(self, words: list) -> list:\n",
    "        \"\"\" Multiline comment after a function will be placed into its documentation.\n",
    "            The function's documentation is accessible by Shift+Tab.\n",
    "            \n",
    "            This function analyses a list of tokens using pymorphy2 and hashes result for faster processing.\n",
    "            \n",
    "            words - list of words.\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for w in words:\n",
    "            if w in self.__cash:\n",
    "                res.append(self.__cash[w])\n",
    "            else:\n",
    "                r = self.__morpho.parse(w)[0].normal_form\n",
    "                res.append(r)\n",
    "                self.__cash[w] = r\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для создания объекта необходимо вызвать функцию с тем же именем, что и имя его типа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast = FasterMorphology()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь у нас есть переменная нужного типа и мы можем обращаться к ее полям, а также вызывать ее методы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 733 ms, sys: 0 ns, total: 733 ms\n",
      "Wall time: 738 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = fast.analyzeWords(words) # Нажмите Shift+Tab чтобы посмотреть документацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['лев',\n",
       " 'николаевич',\n",
       " 'толстой',\n",
       " 'война',\n",
       " 'и',\n",
       " 'мир',\n",
       " 'тот',\n",
       " 'часть',\n",
       " 'первый',\n",
       " 'быть']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь еще раз то же самое, но с заполненным кешем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 74.6 ms, sys: 3.54 ms, total: 78.1 ms\n",
      "Wall time: 77.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = fast.analyzeWords(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ускорение еще в 10 раз!<br>\n",
    "Правда, заодно мы выяснили, что слова надо приводить к единому написанию, устраняя, например, заглавные буквы. Повторим эксперимент без них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "words2 = [w[0].lower() for w in re.findall(\"([А-ЯЁа-яё]+(-[А-ЯЁа-яё]+)*)\", textWP)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast2 = FasterMorphology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 860 ms, sys: 0 ns, total: 860 ms\n",
      "Wall time: 860 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res = fast2.analyzeWords(words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем следующую модификацию нашего класса. Мы хотим, чтобы пользователь мог выбирать с какой системой морфологического анализа работать - PyMorphy или MyStem. Для этого мы создадим две функции каждая из которых отвечает за свою систему. Обе эти функции будут обеспечивать унифицированный интерфейс, принимая на вход список токенов и выдавая список начальных форм. Вообще-то следовало бы передавать на вход строку с текстом, чтобы в случае PyMorphy самим проводить токенизацию, но нам отчего-то захотелось сохранить предыдущий интерфейс (унаследованная система?). \n",
    "\n",
    "В конструкторе (функция `__init__`) мы создадим объект морфологии нужного типа и сохраним в свойство класса `self.analyzeWords` функцию анализа, соответствующую переданному пользователем парамету. Теперь для проведения морфологического анализа необходимо вызвать `self.analyzeWords`, но при этом соверешнно не обязательно знать каким образом был проинициализирован объект нашего класса и какую систему морфологического анализа он использует. Унифицированный интерфейс позволяет пользователю не задумываться над этими вопросами, просто получая результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ключевое слово class после которого идет название нашего класса.\n",
    "class FasterMorphologyUnified:\n",
    "    \n",
    "    def __init__(self, dict_type: str =\"PyM\") -> None: # Функция инициализации объекта после его создания.\n",
    "        # Создаем новую морфологию в каждом объекте. \n",
    "        # А вдруг мы будем потом работать с разными языками? У каждого объекта должна быть своя.\n",
    "        if dict_type == 'PyM':\n",
    "            self.morpho = pymorphy2.MorphAnalyzer() \n",
    "            self.cash = {} # Создаем словарь для кеширования.\n",
    "            self.analyzeWords = self.analyzeWordsWithPymorphy\n",
    "        elif dict_type == 'MyS':\n",
    "            self.mystem = pymystem3.Mystem()\n",
    "            self.analyzeWords = self.analyzeWordsWithMystem\n",
    "        # Вообще-то надо предусмотреть вариант, если нам передали какое-то еще значение, которого мы не знаем.\n",
    "        self.mode = dict_type # Сохраним, чтобы потом можно было понять что за словарь использовался.\n",
    "            \n",
    "    def analyzeWordsWithMystem(self, words: list) -> list:\n",
    "        \"\"\" Функция анализа при помощи MyStem.\n",
    "            words - список токенов для анализа.\n",
    "        \"\"\"\n",
    "        text = ' '.join(words)\n",
    "        tokens = self.mystem.analyze(text)\n",
    "        res = []\n",
    "        for t in tokens:\n",
    "            if 'analysis' in t.keys():\n",
    "                if t['analysis'] != []:\n",
    "                    res.append(t['analysis'][0]['lex'] + '_' + t['analysis'][0]['gr'][0])\n",
    "                else:\n",
    "                    res.append(t['text'] + '_' + 'U')\n",
    "        return res    \n",
    "\n",
    "    def analyzeWordsWithPymorphy(self, words: list) -> list:\n",
    "        \"\"\" Функция анализа при помощи PyMorphy.\n",
    "            words - список токенов для анализа.\n",
    "        \"\"\"\n",
    "        res = []\n",
    "        for w in words:\n",
    "            if w in self.cash:\n",
    "                res.append(self.cash[w])\n",
    "            else:\n",
    "                r = self.morpho.parse(w)[0].normal_form\n",
    "                res.append(r)\n",
    "                self.cash[w] = r\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fast3 = FasterMorphologyUnified('PyM')#('MyS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 638 ms, sys: 8.11 ms, total: 647 ms\n",
      "Wall time: 645 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "res=fast3.analyzeWords(words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Упражнение: класс файла, который читает текст в любой кодировке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
